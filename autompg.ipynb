{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POBjtxq2XoGn"
   },
   "source": [
    "# Artificial Neural Networks 2024.1\n",
    "\n",
    "\n",
    "## Implementation of Neural Networks using Scikit Learn\n",
    "\n",
    "* Professor: Elloá B. Guedes - ebgcosta@uea.edu.br\n",
    "\n",
    "* Student: Danilo Bruno da Silva - dbds.eng21@uea.edu.br\n",
    "\n",
    "\n",
    "### Circumstance: Fuel Usage\n",
    "\n",
    "The objective of this practical activity is to use Machine Learning tools in the Python environment, utilizing the pandas and scikit-learn libraries, to predict the fuel consumption of vehicles.\n",
    "\n",
    "### Database\n",
    "\n",
    "Available at https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oHTyZk2XoGv"
   },
   "source": [
    "### Libs\n",
    "\n",
    "By habit, the first cell of the notebook is usually reserved for importing libraries.\n",
    "With each new library added, it is necessary to run the cell for updating and correct execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9408o_qbXoGy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojhujnUjXoGz"
   },
   "source": [
    "### Opening the Dataset\n",
    "\n",
    "Abra o dataset e visualize o seu cabeçalho, isto é, os primeiros exemplos nele contidos.\n",
    "Isto é útil para checar se a importação foi realizada de maneira adequada e se a disposição dos dados está de acordo para os próximos passos do trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UOQD9uQkXoGz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>modelyear</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0       70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5       70.0   \n",
       "2  18.0        8.0         318.0       150.0  3436.0          11.0       70.0   \n",
       "3  16.0        8.0         304.0       150.0  3433.0          12.0       70.0   \n",
       "4  17.0        8.0         302.0       140.0  3449.0          10.5       70.0   \n",
       "\n",
       "   origin                       name  \n",
       "0     1.0  chevrolet chevelle malibu  \n",
       "1     1.0          buick skylark 320  \n",
       "2     1.0         plymouth satellite  \n",
       "3     1.0              amc rebel sst  \n",
       "4     1.0                ford torino  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"autompg.csv\"\n",
    "data = pd.read_csv(data_path, sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhfWbr9BXoG0"
   },
   "source": [
    "### Knowing the dataset\n",
    "\n",
    "To practice concepts related to exploring the dataset, use the following cells to provide answers to the following questions:\n",
    "\n",
    "1. How many examples are there in the dataset?\n",
    "2. What attributes exist in the dataset?\n",
    "3. What are the names of the cars in the dataset?\n",
    "4. What are the characteristics of the 'chevrolet camaro'?\n",
    "5. What is the average consumption, in gallons per liter, of the cars in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ACmvzPcBXoG0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 406 cars in the dataset\n"
     ]
    }
   ],
   "source": [
    "size = len(data)\n",
    "print(f\"There are {size} cars in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wvFI3a76XoG1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mpg',\n",
       " 'cylinders',\n",
       " 'displacement',\n",
       " 'horsepower',\n",
       " 'weight',\n",
       " 'acceleration',\n",
       " 'modelyear',\n",
       " 'origin',\n",
       " 'name']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = data.columns.tolist()\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2iqkyI2zXoG2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"chevrolet chevelle malibu, buick skylark 320, plymouth satellite, amc rebel sst, ford torino, ford galaxie 500, chevrolet impala, plymouth fury iii, pontiac catalina, amc ambassador dpl, citroen ds-21 pallas, chevrolet chevelle concours (sw), ford torino (sw), plymouth satellite (sw), amc rebel sst (sw), dodge challenger se, plymouth 'cuda 340, ford mustang boss 302, chevrolet monte carlo, buick estate wagon (sw), toyota corona mark ii, plymouth duster, amc hornet, ford maverick, datsun pl510, volkswagen 1131 deluxe sedan, peugeot 504, audi 100 ls, saab 99e, bmw 2002, amc gremlin, ford f250, chevy c20, dodge d200, hi 1200d, chevrolet vega 2300, toyota corona, ford pinto, volkswagen super beetle 117, plymouth satellite custom, ford torino 500, amc matador, pontiac catalina brougham, dodge monaco (sw), ford country squire (sw), pontiac safari (sw), amc hornet sportabout (sw), chevrolet vega (sw), pontiac firebird, ford mustang, mercury capri 2000, opel 1900, peugeot 304, fiat 124b, toyota corolla 1200, datsun 1200, volkswagen model 111, plymouth cricket, toyota corona hardtop, dodge colt hardtop, volkswagen type 3, chevrolet vega, ford pinto runabout, amc ambassador sst, mercury marquis, buick lesabre custom, oldsmobile delta 88 royale, chrysler newport royal, mazda rx2 coupe, amc matador (sw), ford gran torino (sw), plymouth satellite custom (sw), volvo 145e (sw), volkswagen 411 (sw), peugeot 504 (sw), renault 12 (sw), ford pinto (sw), datsun 510 (sw), toyouta corona mark ii (sw), dodge colt (sw), toyota corolla 1600 (sw), buick century 350, chevrolet malibu, ford gran torino, dodge coronet custom, mercury marquis brougham, chevrolet caprice classic, ford ltd, plymouth fury gran sedan, chrysler new yorker brougham, buick electra 225 custom, amc ambassador brougham, plymouth valiant, chevrolet nova custom, volkswagen super beetle, ford country, plymouth custom suburb, oldsmobile vista cruiser, toyota carina, datsun 610, maxda rx3, mercury capri v6, fiat 124 sport coupe, chevrolet monte carlo s, pontiac grand prix, fiat 128, opel manta, audi 100ls, volvo 144ea, dodge dart custom, saab 99le, toyota mark ii, oldsmobile omega, chevrolet nova, datsun b210, chevrolet chevelle malibu classic, plymouth satellite sebring, buick century luxus (sw), dodge coronet custom (sw), audi fox, volkswagen dasher, datsun 710, dodge colt, fiat 124 tc, honda civic, subaru, fiat x1.9, plymouth valiant custom, mercury monarch, chevrolet bel air, plymouth grand fury, buick century, chevroelt chevelle malibu, plymouth fury, buick skyhawk, chevrolet monza 2+2, ford mustang ii, toyota corolla, pontiac astro, volkswagen rabbit, amc pacer, volvo 244dl, honda civic cvcc, fiat 131, capri ii, renault 12tl, dodge coronet brougham, chevrolet chevette, chevrolet woody, vw rabbit, dodge aspen se, ford granada ghia, pontiac ventura sj, amc pacer d/l, datsun b-210, volvo 245, plymouth volare premier v8, mercedes-benz 280s, cadillac seville, chevy c10, ford f108, dodge d100, honda accord cvcc, buick opel isuzu deluxe, renault 5 gtl, plymouth arrow gs, datsun f-10 hatchback, oldsmobile cutlass supreme, dodge monaco brougham, mercury cougar brougham, chevrolet concours, buick skylark, plymouth volare custom, ford granada, pontiac grand prix lj, chevrolet monte carlo landau, chrysler cordoba, ford thunderbird, volkswagen rabbit custom, pontiac sunbird coupe, toyota corolla liftback, ford mustang ii 2+2, dodge colt m/m, subaru dl, datsun 810, bmw 320i, mazda rx-4, volkswagen rabbit custom diesel, ford fiesta, mazda glc deluxe, datsun b210 gx, oldsmobile cutlass salon brougham, dodge diplomat, mercury monarch ghia, pontiac phoenix lj, ford fairmont (auto), ford fairmont (man), plymouth volare, amc concord, buick century special, mercury zephyr, dodge aspen, amc concord d/l, buick regal sport coupe (turbo), ford futura, dodge magnum xe, datsun 510, dodge omni, toyota celica gt liftback, plymouth sapporo, oldsmobile starfire sx, datsun 200-sx, audi 5000, volvo 264gl, saab 99gle, peugeot 604sl, volkswagen scirocco, honda accord lx, pontiac lemans v6, mercury zephyr 6, ford fairmont 4, amc concord dl 6, dodge aspen 6, ford ltd landau, mercury grand marquis, dodge st. regis, chevrolet malibu classic (sw), chrysler lebaron town @ country (sw), vw rabbit custom, maxda glc deluxe, dodge colt hatchback custom, amc spirit dl, mercedes benz 300d, cadillac eldorado, plymouth horizon, plymouth horizon tc3, datsun 210, fiat strada custom, buick skylark limited, chevrolet citation, oldsmobile omega brougham, pontiac phoenix, toyota corolla tercel, datsun 310, ford fairmont, audi 4000, toyota corona liftback, mazda 626, datsun 510 hatchback, mazda glc, vw rabbit c (diesel), vw dasher (diesel), audi 5000s (diesel), mercedes-benz 240d, honda civic 1500 gl, renault lecar deluxe, vokswagen rabbit, datsun 280-zx, mazda rx-7 gs, triumph tr7 coupe, ford mustang cobra, honda accord, plymouth reliant, dodge aries wagon (sw), toyota starlet, plymouth champ, honda civic 1300, datsun 210 mpg, toyota tercel, mazda glc 4, plymouth horizon 4, ford escort 4w, ford escort 2h, volkswagen jetta, renault 18i, honda prelude, datsun 200sx, peugeot 505s turbo diesel, saab 900s, volvo diesel, toyota cressida, datsun 810 maxima, oldsmobile cutlass ls, ford granada gl, chrysler lebaron salon, chevrolet cavalier, chevrolet cavalier wagon, chevrolet cavalier 2-door, pontiac j2000 se hatchback, dodge aries se, ford fairmont futura, amc concord dl, volkswagen rabbit l, mazda glc custom l, mazda glc custom, plymouth horizon miser, mercury lynx l, nissan stanza xe, honda civic (auto), datsun 310 gx, buick century limited, oldsmobile cutlass ciera (diesel), chrysler lebaron medallion, ford granada l, toyota celica gt, dodge charger 2.2, chevrolet camaro, ford mustang gl, vw pickup, dodge rampage, ford ranger, chevy s-10\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels = data['name'].unique()\n",
    "\", \".join(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nx5inKKOXoG2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>modelyear</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chevrolet camaro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "400  27.0        4.0         151.0        90.0  2950.0          17.3   \n",
       "\n",
       "     modelyear  origin              name  \n",
       "400       82.0     1.0  chevrolet camaro  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camaro_data = data[data['name'] == 'chevrolet camaro']\n",
    "camaro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6K3-AsfjXoG4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18045655620744197"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gpl'] = 3.78541 / data['mpg'] # Convert from mpg to gpl -->  gpl = 3.78541 / mpg\n",
    "gpl_average_consuption = data['gpl'].mean()\n",
    "gpl_average_consuption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lIL3Mg2XoG4"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "1. There are examples with missing data. For simplicity, remove them from the dataset.\n",
    "2. Exclude the column with car names.\n",
    "3. Convert mpg to km/l knowing that: 1 mpg = 0.425 km/l. Use only two decimal places in this conversion.\n",
    "4. Remove the mpg column and insert the kml column into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oQGw5E7tXoG6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before removing examples with missing data\n",
      "New size after remotion: 392\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size before removing examples with missing data\")\n",
    "data = data.dropna()\n",
    "size = len(data)\n",
    "print(f\"New size after remotion: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7HuQqPnhXoG6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>modelyear</th>\n",
       "      <th>origin</th>\n",
       "      <th>gpl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.252361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0       70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5       70.0   \n",
       "2  18.0        8.0         318.0       150.0  3436.0          11.0       70.0   \n",
       "3  16.0        8.0         304.0       150.0  3433.0          12.0       70.0   \n",
       "4  17.0        8.0         302.0       140.0  3449.0          10.5       70.0   \n",
       "\n",
       "   origin       gpl  \n",
       "0     1.0  0.210301  \n",
       "1     1.0  0.252361  \n",
       "2     1.0  0.210301  \n",
       "3     1.0  0.236588  \n",
       "4     1.0  0.222671  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=['name'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-8sbVF-kXoG7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>modelyear</th>\n",
       "      <th>origin</th>\n",
       "      <th>gpl</th>\n",
       "      <th>kml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210301</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.252361</td>\n",
       "      <td>6.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210301</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236588</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222671</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0       70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5       70.0   \n",
       "2  18.0        8.0         318.0       150.0  3436.0          11.0       70.0   \n",
       "3  16.0        8.0         304.0       150.0  3433.0          12.0       70.0   \n",
       "4  17.0        8.0         302.0       140.0  3449.0          10.5       70.0   \n",
       "\n",
       "   origin       gpl   kml  \n",
       "0     1.0  0.210301  7.65  \n",
       "1     1.0  0.252361  6.38  \n",
       "2     1.0  0.210301  7.65  \n",
       "3     1.0  0.236588  6.80  \n",
       "4     1.0  0.222671  7.22  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['kml'] = (data['mpg'] * 0.425).round(2)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RU_u_f-lXoG7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>modelyear</th>\n",
       "      <th>origin</th>\n",
       "      <th>gpl</th>\n",
       "      <th>kml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210301</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.252361</td>\n",
       "      <td>6.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210301</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236588</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222671</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0        8.0         307.0       130.0  3504.0          12.0       70.0   \n",
       "1        8.0         350.0       165.0  3693.0          11.5       70.0   \n",
       "2        8.0         318.0       150.0  3436.0          11.0       70.0   \n",
       "3        8.0         304.0       150.0  3433.0          12.0       70.0   \n",
       "4        8.0         302.0       140.0  3449.0          10.5       70.0   \n",
       "\n",
       "   origin       gpl   kml  \n",
       "0     1.0  0.210301  7.65  \n",
       "1     1.0  0.252361  6.38  \n",
       "2     1.0  0.210301  7.65  \n",
       "3     1.0  0.236588  6.80  \n",
       "4     1.0  0.222671  7.22  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=[\"mpg\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxUmCsBFXoG8"
   },
   "source": [
    "### Data Organization for Training\n",
    "\n",
    "1.Remove the kml column and assign it to a variable Y.\n",
    "2. Assign the remaining values of the dataset to a variable X.\n",
    "3.Perform a 70/30 holdout split using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "r_9wE-yIXoG8"
   },
   "outputs": [],
   "source": [
    "X = data.drop('kml', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4yewbP4yXoG8"
   },
   "outputs": [],
   "source": [
    "Y = data['kml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Yre7T29cXoG-"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvi_UllWXoG-"
   },
   "source": [
    "### Training a Linear Regression Model\n",
    "\n",
    "1. Import the model from the sklearn library.\n",
    "2. Instantiate the model with default parameters.\n",
    "3. Execute the training algorithm with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "iw0GUa5FXoG_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression = LinearRegression()\n",
    "\n",
    "regression.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvLc1bMxXoHA"
   },
   "source": [
    "### Model Testing\n",
    "\n",
    "Let's observe the model output for a single example from the training data:\n",
    "\n",
    "      . Predictive attributes: X_test[2:3]\n",
    "      . Target attribute: Y_test.iloc[2]\n",
    "      . What is the predicted result for the model, given these predictive attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nWAZcB2vXoHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor properties: \n",
      "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
      "357  34.7        4.0         105.0        63.0  2215.0          14.9   \n",
      "\n",
      "     modelyear  origin      gpl  \n",
      "357       81.0     1.0  0.10909  \n",
      "Aimed propertie: 14.75\n",
      "model result: 14.746984393186235\n"
     ]
    }
   ],
   "source": [
    "properties = X_test.iloc[2:3]\n",
    "aimed_properties = Y_test.iloc[2]\n",
    "\n",
    "result = regression.predict(properties)\n",
    "\n",
    "print(f\"predictor properties: \\n{properties}\")\n",
    "print(f\"Aimed propertie: {aimed_properties}\")\n",
    "print(f\"model result: {result[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdaYrax6XoHA"
   },
   "source": [
    "### Model Testing\n",
    "\n",
    "1.Obtain the R^2 score for the test data:<br>\n",
    "      - Import r2_score from the sklearn.metrics package.<br>\n",
    "      - This score ranges between [0, 1].<br>\n",
    "      - The closer to 1, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2O6CkmnZXoHB"
   },
   "outputs": [],
   "source": [
    "Y_prediction = regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dqouho0SXoHB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999988478347183"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = r2_score(Y_test, Y_prediction)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjuL4GbMXoHB"
   },
   "source": [
    "### Obtaining and Visualizing Residuals\n",
    "\n",
    "A common way to assess how well a model learns certain patterns is by visualizing residuals, which are the differences between predicted and observed values. Adapt the following code to calculate the residuals produced by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GWWfhs-1XoHC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2933483384047465e-05,\n",
       " 1.7597505808180328e-05,\n",
       " 9.093884455236632e-06,\n",
       " 2.2616511952895108e-05,\n",
       " 4.9039089694515476e-05,\n",
       " 3.070380224374192e-05,\n",
       " 2.7559963412069893e-05,\n",
       " 4.202309274229013e-08,\n",
       " 5.034711405109092e-08,\n",
       " 1.988843857318866e-08,\n",
       " 2.236955016985553e-05,\n",
       " 3.299717144486802e-05,\n",
       " 1.0322737958439694e-07,\n",
       " 3.1824315214752106e-05,\n",
       " 2.5056318976450412e-05,\n",
       " 1.54689072422104e-05,\n",
       " 2.852183286788596e-07,\n",
       " 3.1787356790985723e-06,\n",
       " 6.321024370179262e-06,\n",
       " 4.82181202580116e-08,\n",
       " 9.311140346588419e-06,\n",
       " 1.3040030683064519e-08,\n",
       " 3.206479565389075e-08,\n",
       " 6.1472104125191205e-06,\n",
       " 2.571100693130413e-05,\n",
       " 1.6470242476527692e-06,\n",
       " 2.76405583063858e-05,\n",
       " 1.142046675420425e-07,\n",
       " 4.605648721784354e-06,\n",
       " 2.8386299228323525e-05,\n",
       " 5.5192653811510584e-08,\n",
       " 1.286139250205268e-06,\n",
       " 3.6396393343878926e-08,\n",
       " 5.917994905492853e-06,\n",
       " 2.7295439356661538e-05,\n",
       " 2.555607776430499e-06,\n",
       " 2.648807132026458e-06,\n",
       " 8.104735789471083e-09,\n",
       " 7.516232879655033e-08,\n",
       " 4.6767768787466215e-06,\n",
       " 3.099100816317075e-05,\n",
       " 3.849633569039127e-06,\n",
       " 2.6161339098757458e-05,\n",
       " 2.9280396953413928e-08,\n",
       " 2.7449270357862655e-07,\n",
       " 4.351343907658002e-06,\n",
       " 1.2704055418505328e-05,\n",
       " 2.6580805672678606e-05,\n",
       " 6.028035440255919e-06,\n",
       " 4.2888572596877273e-07,\n",
       " 1.8472884153555527e-05,\n",
       " 1.915117281453126e-05,\n",
       " 3.6338819682433336e-07,\n",
       " 7.2582636310318e-06,\n",
       " 2.225623329610347e-05,\n",
       " 1.6575436322124226e-08,\n",
       " 2.460849951240248e-05,\n",
       " 1.413823018034099e-05,\n",
       " 5.494513860571971e-08,\n",
       " 6.750157267760715e-06,\n",
       " 2.118977629522244e-05,\n",
       " 3.0466351931038737e-05,\n",
       " 1.9265885110931136e-08,\n",
       " 2.7572457151239545e-05,\n",
       " 9.695466088503661e-08,\n",
       " 4.409674490336638e-06,\n",
       " 4.471762811662519e-06,\n",
       " 3.6907516731927214e-07,\n",
       " 2.010828400695772e-05,\n",
       " 5.685928738381052e-08,\n",
       " 2.0717257877965293e-09,\n",
       " 9.716864474325237e-08,\n",
       " 2.469199914303107e-05,\n",
       " 2.2780028938454785e-06,\n",
       " 4.984773259916595e-07,\n",
       " 7.887713540343032e-06,\n",
       " 2.3979460938520255e-05,\n",
       " 2.6389973547345656e-05,\n",
       " 3.2925377893066266e-05,\n",
       " 4.9467970925908324e-06,\n",
       " 3.1259208349978255e-05,\n",
       " 1.9190100039707788e-05,\n",
       " 4.9406543641079535e-06,\n",
       " 1.8495006711473494e-05,\n",
       " 1.17698247027502e-06,\n",
       " 3.984964152485218e-08,\n",
       " 1.924032287398597e-05,\n",
       " 1.393061404377451e-08,\n",
       " 2.5571982523094208e-05,\n",
       " 9.061250134894924e-06,\n",
       " 2.309864619982232e-05,\n",
       " 2.6039658199294217e-07,\n",
       " 1.064629529995747e-07,\n",
       " 2.6327233995155555e-05,\n",
       " 3.058568398316223e-07,\n",
       " 2.0955948682596945e-08,\n",
       " 1.1097455561716349e-05,\n",
       " 1.8590430262433637e-07,\n",
       " 2.8543764435246966e-05,\n",
       " 1.8564585480921615e-05,\n",
       " 1.1093420891637397e-09,\n",
       " 3.623399309434989e-05,\n",
       " 1.40707926372012e-07,\n",
       " 4.034191532365226e-09,\n",
       " 1.7241287811648615e-07,\n",
       " 8.576538563140261e-09,\n",
       " 3.934994020056167e-06,\n",
       " 2.0531424263768513e-05,\n",
       " 5.439727472925124e-06,\n",
       " 4.598837140091401e-05,\n",
       " 1.7898152843854108e-07,\n",
       " 2.1904272325784875e-07,\n",
       " 3.348044657810358e-07,\n",
       " 3.1499829413652975e-07,\n",
       " 1.0389248783682572e-08,\n",
       " 8.675807482577182e-06,\n",
       " 4.540993937362462e-06,\n",
       " 9.491804118870546e-06]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garbage = []\n",
    "for (x,y) in zip(Y_test,Y_prediction):\n",
    "    garbage.append((x-y)**2)\n",
    "garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mQofP5ZoXoHC"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_plt = np.array(Y_test)\n",
    "garbage_plt = np.array(garbage)\n",
    "\n",
    "x = [0,int(max(Y_test))]\n",
    "y = [0,0]\n",
    "plt.plot(x,y,linewidth=3)\n",
    "plt.plot(Y_test_plt, garbage_plt,'ro')\n",
    "plt.ylabel('garbage')\n",
    "plt.xlabel('kml')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w9Jzn0VXoHD"
   },
   "source": [
    "### Proposing MLPs\n",
    "Scaling Attributes\n",
    "Training an artificial neural network is more efficient when the input values are small, as this aids in convergence. This is achieved by scaling all attributes to the range [0,1], but it must be done carefully to ensure that information from the test set does not influence training.\n",
    "\n",
    "There are two strategies for scaling: normalization and standardization. Both have unique characteristics, advantages, and limitations, as detailed here: Feature Scaling in Machine Learning: Normalization vs Standardization\n",
    "\n",
    "In our case, we will use standardization. Thus, for the training predictive attributes, i.e., X_train, we will subtract the mean and divide by the standard deviation:\n",
    "\n",
    "X_train_std = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "\n",
    "Similarly, the same must be done for the test predictive attributes, but using standardization relative to the training set:\n",
    "\n",
    "X_test_std = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "\n",
    "If the entire set X is used for standardization, the neural network will receive information from the test set through the mean and variance used to prepare the training data, which is undesirable.\n",
    "\n",
    "### Proposal of a Single-Layer MLP\n",
    "1- Refer to the documentation at MLPClassifier Documentation\n",
    "2 - Train a multilayer perceptron neural network for this problem with a single layer and ten neurons<br>\n",
    "2.1 Use the ReLU activation function<br>\n",
    "2.2 Use the Adam solver<br>\n",
    "2.3 Print the training progress<br>\n",
    "2.4 Set the maximum number of epochs to 300<br>\n",
    "\n",
    "3- Obtain the R^2 score on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9wa55vpDXoHD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 47.18928747\n",
      "Iteration 2, loss = 46.89405539\n",
      "Iteration 3, loss = 46.59420606\n",
      "Iteration 4, loss = 46.29897142\n",
      "Iteration 5, loss = 46.00449968\n",
      "Iteration 6, loss = 45.71738774\n",
      "Iteration 7, loss = 45.42615412\n",
      "Iteration 8, loss = 45.13663941\n",
      "Iteration 9, loss = 44.86058937\n",
      "Iteration 10, loss = 44.57579755\n",
      "Iteration 11, loss = 44.29192871\n",
      "Iteration 12, loss = 44.01019942\n",
      "Iteration 13, loss = 43.73224150\n",
      "Iteration 14, loss = 43.45435050\n",
      "Iteration 15, loss = 43.18207974\n",
      "Iteration 16, loss = 42.90500942\n",
      "Iteration 17, loss = 42.62281049\n",
      "Iteration 18, loss = 42.35904953\n",
      "Iteration 19, loss = 42.08403816\n",
      "Iteration 20, loss = 41.81004234\n",
      "Iteration 21, loss = 41.54002966\n",
      "Iteration 22, loss = 41.27122732\n",
      "Iteration 23, loss = 41.00193898\n",
      "Iteration 24, loss = 40.73572570\n",
      "Iteration 25, loss = 40.45985684\n",
      "Iteration 26, loss = 40.19450427\n",
      "Iteration 27, loss = 39.92533430\n",
      "Iteration 28, loss = 39.65600322\n",
      "Iteration 29, loss = 39.39127044\n",
      "Iteration 30, loss = 39.11805907\n",
      "Iteration 31, loss = 38.85473150\n",
      "Iteration 32, loss = 38.58933335\n",
      "Iteration 33, loss = 38.31562157\n",
      "Iteration 34, loss = 38.05762386\n",
      "Iteration 35, loss = 37.78110644\n",
      "Iteration 36, loss = 37.51702941\n",
      "Iteration 37, loss = 37.25144250\n",
      "Iteration 38, loss = 36.98025665\n",
      "Iteration 39, loss = 36.70752143\n",
      "Iteration 40, loss = 36.44000922\n",
      "Iteration 41, loss = 36.16787660\n",
      "Iteration 42, loss = 35.89577086\n",
      "Iteration 43, loss = 35.62886945\n",
      "Iteration 44, loss = 35.35725300\n",
      "Iteration 45, loss = 35.08455379\n",
      "Iteration 46, loss = 34.81730743\n",
      "Iteration 47, loss = 34.54483631\n",
      "Iteration 48, loss = 34.27838044\n",
      "Iteration 49, loss = 34.00225165\n",
      "Iteration 50, loss = 33.73411885\n",
      "Iteration 51, loss = 33.46495640\n",
      "Iteration 52, loss = 33.18920882\n",
      "Iteration 53, loss = 32.92937752\n",
      "Iteration 54, loss = 32.64542945\n",
      "Iteration 55, loss = 32.37659804\n",
      "Iteration 56, loss = 32.11049511\n",
      "Iteration 57, loss = 31.84016576\n",
      "Iteration 58, loss = 31.56457572\n",
      "Iteration 59, loss = 31.29368671\n",
      "Iteration 60, loss = 31.02543482\n",
      "Iteration 61, loss = 30.75257599\n",
      "Iteration 62, loss = 30.48233476\n",
      "Iteration 63, loss = 30.21505658\n",
      "Iteration 64, loss = 29.94020700\n",
      "Iteration 65, loss = 29.66995853\n",
      "Iteration 66, loss = 29.39919923\n",
      "Iteration 67, loss = 29.13012980\n",
      "Iteration 68, loss = 28.86476001\n",
      "Iteration 69, loss = 28.59459934\n",
      "Iteration 70, loss = 28.32501465\n",
      "Iteration 71, loss = 28.06179784\n",
      "Iteration 72, loss = 27.79517106\n",
      "Iteration 73, loss = 27.53196110\n",
      "Iteration 74, loss = 27.26609037\n",
      "Iteration 75, loss = 27.00072563\n",
      "Iteration 76, loss = 26.74449109\n",
      "Iteration 77, loss = 26.47668936\n",
      "Iteration 78, loss = 26.22308571\n",
      "Iteration 79, loss = 25.95944968\n",
      "Iteration 80, loss = 25.69903706\n",
      "Iteration 81, loss = 25.44168540\n",
      "Iteration 82, loss = 25.18796576\n",
      "Iteration 83, loss = 24.93215696\n",
      "Iteration 84, loss = 24.67569099\n",
      "Iteration 85, loss = 24.42151501\n",
      "Iteration 86, loss = 24.16762599\n",
      "Iteration 87, loss = 23.92047764\n",
      "Iteration 88, loss = 23.66867926\n",
      "Iteration 89, loss = 23.41428796\n",
      "Iteration 90, loss = 23.17048499\n",
      "Iteration 91, loss = 22.92241861\n",
      "Iteration 92, loss = 22.67377934\n",
      "Iteration 93, loss = 22.43182912\n",
      "Iteration 94, loss = 22.18796207\n",
      "Iteration 95, loss = 21.94475938\n",
      "Iteration 96, loss = 21.70648275\n",
      "Iteration 97, loss = 21.46304305\n",
      "Iteration 98, loss = 21.23244534\n",
      "Iteration 99, loss = 20.99202365\n",
      "Iteration 100, loss = 20.76303711\n",
      "Iteration 101, loss = 20.52805531\n",
      "Iteration 102, loss = 20.30295771\n",
      "Iteration 103, loss = 20.07459236\n",
      "Iteration 104, loss = 19.84514322\n",
      "Iteration 105, loss = 19.62845093\n",
      "Iteration 106, loss = 19.40342461\n",
      "Iteration 107, loss = 19.18847836\n",
      "Iteration 108, loss = 18.97065366\n",
      "Iteration 109, loss = 18.75831839\n",
      "Iteration 110, loss = 18.54691678\n",
      "Iteration 111, loss = 18.33488257\n",
      "Iteration 112, loss = 18.12718799\n",
      "Iteration 113, loss = 17.92611404\n",
      "Iteration 114, loss = 17.72150298\n",
      "Iteration 115, loss = 17.52194115\n",
      "Iteration 116, loss = 17.32071331\n",
      "Iteration 117, loss = 17.12473595\n",
      "Iteration 118, loss = 16.93256144\n",
      "Iteration 119, loss = 16.73875632\n",
      "Iteration 120, loss = 16.54959018\n",
      "Iteration 121, loss = 16.36224280\n",
      "Iteration 122, loss = 16.18011631\n",
      "Iteration 123, loss = 15.99746912\n",
      "Iteration 124, loss = 15.81541859\n",
      "Iteration 125, loss = 15.63838734\n",
      "Iteration 126, loss = 15.46521951\n",
      "Iteration 127, loss = 15.29105505\n",
      "Iteration 128, loss = 15.12253018\n",
      "Iteration 129, loss = 14.95278751\n",
      "Iteration 130, loss = 14.78679851\n",
      "Iteration 131, loss = 14.62339417\n",
      "Iteration 132, loss = 14.46116693\n",
      "Iteration 133, loss = 14.30276795\n",
      "Iteration 134, loss = 14.14466346\n",
      "Iteration 135, loss = 13.98971205\n",
      "Iteration 136, loss = 13.83606632\n",
      "Iteration 137, loss = 13.68549448\n",
      "Iteration 138, loss = 13.53538388\n",
      "Iteration 139, loss = 13.38986684\n",
      "Iteration 140, loss = 13.24512109\n",
      "Iteration 141, loss = 13.10218796\n",
      "Iteration 142, loss = 12.96216554\n",
      "Iteration 143, loss = 12.82376733\n",
      "Iteration 144, loss = 12.68865623\n",
      "Iteration 145, loss = 12.55368878\n",
      "Iteration 146, loss = 12.42068433\n",
      "Iteration 147, loss = 12.29183667\n",
      "Iteration 148, loss = 12.16496250\n",
      "Iteration 149, loss = 12.03860751\n",
      "Iteration 150, loss = 11.91660136\n",
      "Iteration 151, loss = 11.79510781\n",
      "Iteration 152, loss = 11.67662666\n",
      "Iteration 153, loss = 11.56156306\n",
      "Iteration 154, loss = 11.44915891\n",
      "Iteration 155, loss = 11.33578385\n",
      "Iteration 156, loss = 11.22464779\n",
      "Iteration 157, loss = 11.11927855\n",
      "Iteration 158, loss = 11.01237324\n",
      "Iteration 159, loss = 10.90709520\n",
      "Iteration 160, loss = 10.80567878\n",
      "Iteration 161, loss = 10.70277109\n",
      "Iteration 162, loss = 10.60447824\n",
      "Iteration 163, loss = 10.50515758\n",
      "Iteration 164, loss = 10.40881974\n",
      "Iteration 165, loss = 10.31662407\n",
      "Iteration 166, loss = 10.22078187\n",
      "Iteration 167, loss = 10.12926055\n",
      "Iteration 168, loss = 10.03901214\n",
      "Iteration 169, loss = 9.95312578\n",
      "Iteration 170, loss = 9.86538927\n",
      "Iteration 171, loss = 9.78173550\n",
      "Iteration 172, loss = 9.69779187\n",
      "Iteration 173, loss = 9.61960944\n",
      "Iteration 174, loss = 9.53839837\n",
      "Iteration 175, loss = 9.46085001\n",
      "Iteration 176, loss = 9.38591875\n",
      "Iteration 177, loss = 9.31144352\n",
      "Iteration 178, loss = 9.23768765\n",
      "Iteration 179, loss = 9.16605395\n",
      "Iteration 180, loss = 9.09561907\n",
      "Iteration 181, loss = 9.02783461\n",
      "Iteration 182, loss = 8.95904380\n",
      "Iteration 183, loss = 8.89187050\n",
      "Iteration 184, loss = 8.82920354\n",
      "Iteration 185, loss = 8.76351099\n",
      "Iteration 186, loss = 8.70176559\n",
      "Iteration 187, loss = 8.64112059\n",
      "Iteration 188, loss = 8.58105017\n",
      "Iteration 189, loss = 8.52132113\n",
      "Iteration 190, loss = 8.46530035\n",
      "Iteration 191, loss = 8.40814605\n",
      "Iteration 192, loss = 8.35249531\n",
      "Iteration 193, loss = 8.29949144\n",
      "Iteration 194, loss = 8.24653802\n",
      "Iteration 195, loss = 8.19254618\n",
      "Iteration 196, loss = 8.14284783\n",
      "Iteration 197, loss = 8.09221289\n",
      "Iteration 198, loss = 8.04231857\n",
      "Iteration 199, loss = 7.99582547\n",
      "Iteration 200, loss = 7.94528139\n",
      "Iteration 201, loss = 7.89994545\n",
      "Iteration 202, loss = 7.85506356\n",
      "Iteration 203, loss = 7.80884084\n",
      "Iteration 204, loss = 7.76655095\n",
      "Iteration 205, loss = 7.72283310\n",
      "Iteration 206, loss = 7.67987781\n",
      "Iteration 207, loss = 7.64047822\n",
      "Iteration 208, loss = 7.59804360\n",
      "Iteration 209, loss = 7.55937347\n",
      "Iteration 210, loss = 7.51965373\n",
      "Iteration 211, loss = 7.48091632\n",
      "Iteration 212, loss = 7.44240710\n",
      "Iteration 213, loss = 7.40554100\n",
      "Iteration 214, loss = 7.36890738\n",
      "Iteration 215, loss = 7.33345651\n",
      "Iteration 216, loss = 7.29795975\n",
      "Iteration 217, loss = 7.26385306\n",
      "Iteration 218, loss = 7.22974026\n",
      "Iteration 219, loss = 7.19708266\n",
      "Iteration 220, loss = 7.16485974\n",
      "Iteration 221, loss = 7.13252602\n",
      "Iteration 222, loss = 7.10200112\n",
      "Iteration 223, loss = 7.07103654\n",
      "Iteration 224, loss = 7.04020935\n",
      "Iteration 225, loss = 7.01097138\n",
      "Iteration 226, loss = 6.98104769\n",
      "Iteration 227, loss = 6.95247030\n",
      "Iteration 228, loss = 6.92366963\n",
      "Iteration 229, loss = 6.89513377\n",
      "Iteration 230, loss = 6.86772426\n",
      "Iteration 231, loss = 6.84025500\n",
      "Iteration 232, loss = 6.81304697\n",
      "Iteration 233, loss = 6.78728901\n",
      "Iteration 234, loss = 6.75975048\n",
      "Iteration 235, loss = 6.73432798\n",
      "Iteration 236, loss = 6.70889131\n",
      "Iteration 237, loss = 6.68312768\n",
      "Iteration 238, loss = 6.65791534\n",
      "Iteration 239, loss = 6.63324136\n",
      "Iteration 240, loss = 6.60873026\n",
      "Iteration 241, loss = 6.58367321\n",
      "Iteration 242, loss = 6.55994708\n",
      "Iteration 243, loss = 6.53607529\n",
      "Iteration 244, loss = 6.51243060\n",
      "Iteration 245, loss = 6.48925386\n",
      "Iteration 246, loss = 6.46545378\n",
      "Iteration 247, loss = 6.44356218\n",
      "Iteration 248, loss = 6.42043039\n",
      "Iteration 249, loss = 6.39740071\n",
      "Iteration 250, loss = 6.37508244\n",
      "Iteration 251, loss = 6.35315689\n",
      "Iteration 252, loss = 6.33069799\n",
      "Iteration 253, loss = 6.30960726\n",
      "Iteration 254, loss = 6.28741184\n",
      "Iteration 255, loss = 6.26560000\n",
      "Iteration 256, loss = 6.24538709\n",
      "Iteration 257, loss = 6.22363648\n",
      "Iteration 258, loss = 6.20237992\n",
      "Iteration 259, loss = 6.18188964\n",
      "Iteration 260, loss = 6.16186241\n",
      "Iteration 261, loss = 6.14147122\n",
      "Iteration 262, loss = 6.12115360\n",
      "Iteration 263, loss = 6.10184761\n",
      "Iteration 264, loss = 6.08231464\n",
      "Iteration 265, loss = 6.06278267\n",
      "Iteration 266, loss = 6.04447513\n",
      "Iteration 267, loss = 6.02507948\n",
      "Iteration 268, loss = 6.00720573\n",
      "Iteration 269, loss = 5.98890779\n",
      "Iteration 270, loss = 5.97107607\n",
      "Iteration 271, loss = 5.95296925\n",
      "Iteration 272, loss = 5.93558902\n",
      "Iteration 273, loss = 5.91777260\n",
      "Iteration 274, loss = 5.90069248\n",
      "Iteration 275, loss = 5.88295047\n",
      "Iteration 276, loss = 5.86643171\n",
      "Iteration 277, loss = 5.84904551\n",
      "Iteration 278, loss = 5.83253685\n",
      "Iteration 279, loss = 5.81603035\n",
      "Iteration 280, loss = 5.79923534\n",
      "Iteration 281, loss = 5.78292692\n",
      "Iteration 282, loss = 5.76631539\n",
      "Iteration 283, loss = 5.75010574\n",
      "Iteration 284, loss = 5.73451041\n",
      "Iteration 285, loss = 5.71848711\n",
      "Iteration 286, loss = 5.70231364\n",
      "Iteration 287, loss = 5.68713199\n",
      "Iteration 288, loss = 5.67119361\n",
      "Iteration 289, loss = 5.65571350\n",
      "Iteration 290, loss = 5.64028825\n",
      "Iteration 291, loss = 5.62557068\n",
      "Iteration 292, loss = 5.61025218\n",
      "Iteration 293, loss = 5.59445116\n",
      "Iteration 294, loss = 5.58076097\n",
      "Iteration 295, loss = 5.56479780\n",
      "Iteration 296, loss = 5.54993854\n",
      "Iteration 297, loss = 5.53522768\n",
      "Iteration 298, loss = 5.52059122\n",
      "Iteration 299, loss = 5.50585708\n",
      "Iteration 300, loss = 5.49133163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.3208321841604178"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_default_train = (X_train - X_train.mean()) / X_train.std()\n",
    "x_default_test = (X_test - X_train.mean()) / X_train.std()\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=300, verbose=True)\n",
    "mlp.fit(x_default_train, Y_train)\n",
    "\n",
    "Y_pred = mlp.predict(x_default_test)\n",
    "\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "YuTw4WveXoHD",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Proposing a MLP with Two Hidden Layers\n",
    "\n",
    "1. Train a multilayer perceptron neural network for this problem with two hidden layers, with a number of neurons of your choice<br>\n",
    "1.1 Use the ReLU activation function<br>\n",
    "1.2 Use the Adam solver<br>\n",
    "1.3 Print the training progress<br>\n",
    "1.4 Set the maximum number of epochs to 300<br>\n",
    "\n",
    "2. Obtain the R^2 score on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "doX0hENCXoHD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 59.09564460\n",
      "Iteration 2, loss = 57.83123685\n",
      "Iteration 3, loss = 56.57508171\n",
      "Iteration 4, loss = 55.36526112\n",
      "Iteration 5, loss = 54.17654759\n",
      "Iteration 6, loss = 53.02381160\n",
      "Iteration 7, loss = 51.89371185\n",
      "Iteration 8, loss = 50.78836465\n",
      "Iteration 9, loss = 49.65939019\n",
      "Iteration 10, loss = 48.57941113\n",
      "Iteration 11, loss = 47.48008439\n",
      "Iteration 12, loss = 46.37747621\n",
      "Iteration 13, loss = 45.26416563\n",
      "Iteration 14, loss = 44.13412164\n",
      "Iteration 15, loss = 42.94920534\n",
      "Iteration 16, loss = 41.77050762\n",
      "Iteration 17, loss = 40.50066721\n",
      "Iteration 18, loss = 39.21670833\n",
      "Iteration 19, loss = 37.87689369\n",
      "Iteration 20, loss = 36.53350817\n",
      "Iteration 21, loss = 35.10334422\n",
      "Iteration 22, loss = 33.70808109\n",
      "Iteration 23, loss = 32.26159182\n",
      "Iteration 24, loss = 30.78888404\n",
      "Iteration 25, loss = 29.27574125\n",
      "Iteration 26, loss = 27.84250160\n",
      "Iteration 27, loss = 26.33681336\n",
      "Iteration 28, loss = 24.87367843\n",
      "Iteration 29, loss = 23.40458620\n",
      "Iteration 30, loss = 21.95709596\n",
      "Iteration 31, loss = 20.58198170\n",
      "Iteration 32, loss = 19.20051852\n",
      "Iteration 33, loss = 17.83326236\n",
      "Iteration 34, loss = 16.55114719\n",
      "Iteration 35, loss = 15.30262640\n",
      "Iteration 36, loss = 14.09742313\n",
      "Iteration 37, loss = 12.96535326\n",
      "Iteration 38, loss = 11.90013030\n",
      "Iteration 39, loss = 10.90773665\n",
      "Iteration 40, loss = 9.98925762\n",
      "Iteration 41, loss = 9.13027175\n",
      "Iteration 42, loss = 8.38342786\n",
      "Iteration 43, loss = 7.66679106\n",
      "Iteration 44, loss = 7.08896199\n",
      "Iteration 45, loss = 6.53506553\n",
      "Iteration 46, loss = 6.10454011\n",
      "Iteration 47, loss = 5.71208122\n",
      "Iteration 48, loss = 5.37793872\n",
      "Iteration 49, loss = 5.10543080\n",
      "Iteration 50, loss = 4.89730054\n",
      "Iteration 51, loss = 4.70725437\n",
      "Iteration 52, loss = 4.55516314\n",
      "Iteration 53, loss = 4.42961689\n",
      "Iteration 54, loss = 4.31800508\n",
      "Iteration 55, loss = 4.21415020\n",
      "Iteration 56, loss = 4.12332188\n",
      "Iteration 57, loss = 4.02927338\n",
      "Iteration 58, loss = 3.94580422\n",
      "Iteration 59, loss = 3.85922318\n",
      "Iteration 60, loss = 3.77025642\n",
      "Iteration 61, loss = 3.68446272\n",
      "Iteration 62, loss = 3.59857448\n",
      "Iteration 63, loss = 3.51242685\n",
      "Iteration 64, loss = 3.42920947\n",
      "Iteration 65, loss = 3.34250382\n",
      "Iteration 66, loss = 3.26480065\n",
      "Iteration 67, loss = 3.18234445\n",
      "Iteration 68, loss = 3.10666004\n",
      "Iteration 69, loss = 3.02848139\n",
      "Iteration 70, loss = 2.95795610\n",
      "Iteration 71, loss = 2.88568255\n",
      "Iteration 72, loss = 2.81437679\n",
      "Iteration 73, loss = 2.74827823\n",
      "Iteration 74, loss = 2.67836050\n",
      "Iteration 75, loss = 2.61229045\n",
      "Iteration 76, loss = 2.54726839\n",
      "Iteration 77, loss = 2.48255677\n",
      "Iteration 78, loss = 2.41757283\n",
      "Iteration 79, loss = 2.35845734\n",
      "Iteration 80, loss = 2.29535300\n",
      "Iteration 81, loss = 2.23830149\n",
      "Iteration 82, loss = 2.17795336\n",
      "Iteration 83, loss = 2.12250293\n",
      "Iteration 84, loss = 2.06718752\n",
      "Iteration 85, loss = 2.01481877\n",
      "Iteration 86, loss = 1.96343938\n",
      "Iteration 87, loss = 1.91259285\n",
      "Iteration 88, loss = 1.86374479\n",
      "Iteration 89, loss = 1.81593488\n",
      "Iteration 90, loss = 1.76913287\n",
      "Iteration 91, loss = 1.72534870\n",
      "Iteration 92, loss = 1.68277446\n",
      "Iteration 93, loss = 1.63908412\n",
      "Iteration 94, loss = 1.59876851\n",
      "Iteration 95, loss = 1.56029349\n",
      "Iteration 96, loss = 1.52029073\n",
      "Iteration 97, loss = 1.48341257\n",
      "Iteration 98, loss = 1.44771082\n",
      "Iteration 99, loss = 1.41248154\n",
      "Iteration 100, loss = 1.37870193\n",
      "Iteration 101, loss = 1.34710762\n",
      "Iteration 102, loss = 1.31459661\n",
      "Iteration 103, loss = 1.28312067\n",
      "Iteration 104, loss = 1.25410542\n",
      "Iteration 105, loss = 1.22477870\n",
      "Iteration 106, loss = 1.19718208\n",
      "Iteration 107, loss = 1.17096322\n",
      "Iteration 108, loss = 1.14501781\n",
      "Iteration 109, loss = 1.11855108\n",
      "Iteration 110, loss = 1.09467813\n",
      "Iteration 111, loss = 1.07173551\n",
      "Iteration 112, loss = 1.04878743\n",
      "Iteration 113, loss = 1.02673405\n",
      "Iteration 114, loss = 1.00579957\n",
      "Iteration 115, loss = 0.98431311\n",
      "Iteration 116, loss = 0.96443360\n",
      "Iteration 117, loss = 0.94589621\n",
      "Iteration 118, loss = 0.92707240\n",
      "Iteration 119, loss = 0.90818397\n",
      "Iteration 120, loss = 0.89016540\n",
      "Iteration 121, loss = 0.87295303\n",
      "Iteration 122, loss = 0.85687556\n",
      "Iteration 123, loss = 0.84010731\n",
      "Iteration 124, loss = 0.82480522\n",
      "Iteration 125, loss = 0.80968457\n",
      "Iteration 126, loss = 0.79485241\n",
      "Iteration 127, loss = 0.78038513\n",
      "Iteration 128, loss = 0.76749203\n",
      "Iteration 129, loss = 0.75426971\n",
      "Iteration 130, loss = 0.74170522\n",
      "Iteration 131, loss = 0.72890696\n",
      "Iteration 132, loss = 0.71764558\n",
      "Iteration 133, loss = 0.70602404\n",
      "Iteration 134, loss = 0.69465187\n",
      "Iteration 135, loss = 0.68422139\n",
      "Iteration 136, loss = 0.67364649\n",
      "Iteration 137, loss = 0.66366707\n",
      "Iteration 138, loss = 0.65360952\n",
      "Iteration 139, loss = 0.64383379\n",
      "Iteration 140, loss = 0.63439333\n",
      "Iteration 141, loss = 0.62512923\n",
      "Iteration 142, loss = 0.61597457\n",
      "Iteration 143, loss = 0.60649849\n",
      "Iteration 144, loss = 0.59741058\n",
      "Iteration 145, loss = 0.58875530\n",
      "Iteration 146, loss = 0.57992084\n",
      "Iteration 147, loss = 0.57124429\n",
      "Iteration 148, loss = 0.56309085\n",
      "Iteration 149, loss = 0.55551502\n",
      "Iteration 150, loss = 0.54781735\n",
      "Iteration 151, loss = 0.54083710\n",
      "Iteration 152, loss = 0.53360767\n",
      "Iteration 153, loss = 0.52687862\n",
      "Iteration 154, loss = 0.51989621\n",
      "Iteration 155, loss = 0.51334856\n",
      "Iteration 156, loss = 0.50667220\n",
      "Iteration 157, loss = 0.50070696\n",
      "Iteration 158, loss = 0.49464562\n",
      "Iteration 159, loss = 0.48864374\n",
      "Iteration 160, loss = 0.48296866\n",
      "Iteration 161, loss = 0.47758119\n",
      "Iteration 162, loss = 0.47237018\n",
      "Iteration 163, loss = 0.46700521\n",
      "Iteration 164, loss = 0.46178938\n",
      "Iteration 165, loss = 0.45666076\n",
      "Iteration 166, loss = 0.45210964\n",
      "Iteration 167, loss = 0.44719776\n",
      "Iteration 168, loss = 0.44259503\n",
      "Iteration 169, loss = 0.43788424\n",
      "Iteration 170, loss = 0.43321631\n",
      "Iteration 171, loss = 0.42893852\n",
      "Iteration 172, loss = 0.42458051\n",
      "Iteration 173, loss = 0.42045139\n",
      "Iteration 174, loss = 0.41632354\n",
      "Iteration 175, loss = 0.41213326\n",
      "Iteration 176, loss = 0.40829259\n",
      "Iteration 177, loss = 0.40431294\n",
      "Iteration 178, loss = 0.40041951\n",
      "Iteration 179, loss = 0.39685228\n",
      "Iteration 180, loss = 0.39328542\n",
      "Iteration 181, loss = 0.38951101\n",
      "Iteration 182, loss = 0.38621272\n",
      "Iteration 183, loss = 0.38272162\n",
      "Iteration 184, loss = 0.37936211\n",
      "Iteration 185, loss = 0.37618185\n",
      "Iteration 186, loss = 0.37302393\n",
      "Iteration 187, loss = 0.36971600\n",
      "Iteration 188, loss = 0.36675847\n",
      "Iteration 189, loss = 0.36373250\n",
      "Iteration 190, loss = 0.36082308\n",
      "Iteration 191, loss = 0.35795496\n",
      "Iteration 192, loss = 0.35494791\n",
      "Iteration 193, loss = 0.35214163\n",
      "Iteration 194, loss = 0.34922084\n",
      "Iteration 195, loss = 0.34660872\n",
      "Iteration 196, loss = 0.34397472\n",
      "Iteration 197, loss = 0.34129111\n",
      "Iteration 198, loss = 0.33859296\n",
      "Iteration 199, loss = 0.33575512\n",
      "Iteration 200, loss = 0.33328962\n",
      "Iteration 201, loss = 0.33049465\n",
      "Iteration 202, loss = 0.32772641\n",
      "Iteration 203, loss = 0.32534482\n",
      "Iteration 204, loss = 0.32268884\n",
      "Iteration 205, loss = 0.32015538\n",
      "Iteration 206, loss = 0.31786527\n",
      "Iteration 207, loss = 0.31534610\n",
      "Iteration 208, loss = 0.31282705\n",
      "Iteration 209, loss = 0.31037944\n",
      "Iteration 210, loss = 0.30807870\n",
      "Iteration 211, loss = 0.30579679\n",
      "Iteration 212, loss = 0.30332639\n",
      "Iteration 213, loss = 0.30103521\n",
      "Iteration 214, loss = 0.29879491\n",
      "Iteration 215, loss = 0.29660564\n",
      "Iteration 216, loss = 0.29446410\n",
      "Iteration 217, loss = 0.29243016\n",
      "Iteration 218, loss = 0.29024901\n",
      "Iteration 219, loss = 0.28809176\n",
      "Iteration 220, loss = 0.28595834\n",
      "Iteration 221, loss = 0.28378087\n",
      "Iteration 222, loss = 0.28160880\n",
      "Iteration 223, loss = 0.27938342\n",
      "Iteration 224, loss = 0.27757572\n",
      "Iteration 225, loss = 0.27575159\n",
      "Iteration 226, loss = 0.27346359\n",
      "Iteration 227, loss = 0.27156919\n",
      "Iteration 228, loss = 0.26969280\n",
      "Iteration 229, loss = 0.26775765\n",
      "Iteration 230, loss = 0.26578299\n",
      "Iteration 231, loss = 0.26394027\n",
      "Iteration 232, loss = 0.26206856\n",
      "Iteration 233, loss = 0.26026289\n",
      "Iteration 234, loss = 0.25847080\n",
      "Iteration 235, loss = 0.25661150\n",
      "Iteration 236, loss = 0.25506036\n",
      "Iteration 237, loss = 0.25337513\n",
      "Iteration 238, loss = 0.25183097\n",
      "Iteration 239, loss = 0.24994274\n",
      "Iteration 240, loss = 0.24820978\n",
      "Iteration 241, loss = 0.24649987\n",
      "Iteration 242, loss = 0.24479125\n",
      "Iteration 243, loss = 0.24314670\n",
      "Iteration 244, loss = 0.24150840\n",
      "Iteration 245, loss = 0.23985614\n",
      "Iteration 246, loss = 0.23816070\n",
      "Iteration 247, loss = 0.23655532\n",
      "Iteration 248, loss = 0.23502363\n",
      "Iteration 249, loss = 0.23340886\n",
      "Iteration 250, loss = 0.23179211\n",
      "Iteration 251, loss = 0.23001015\n",
      "Iteration 252, loss = 0.22849646\n",
      "Iteration 253, loss = 0.22693237\n",
      "Iteration 254, loss = 0.22533341\n",
      "Iteration 255, loss = 0.22371161\n",
      "Iteration 256, loss = 0.22218358\n",
      "Iteration 257, loss = 0.22055457\n",
      "Iteration 258, loss = 0.21917249\n",
      "Iteration 259, loss = 0.21774750\n",
      "Iteration 260, loss = 0.21624785\n",
      "Iteration 261, loss = 0.21466137\n",
      "Iteration 262, loss = 0.21309729\n",
      "Iteration 263, loss = 0.21162721\n",
      "Iteration 264, loss = 0.21006363\n",
      "Iteration 265, loss = 0.20871220\n",
      "Iteration 266, loss = 0.20714036\n",
      "Iteration 267, loss = 0.20568139\n",
      "Iteration 268, loss = 0.20425085\n",
      "Iteration 269, loss = 0.20296006\n",
      "Iteration 270, loss = 0.20176800\n",
      "Iteration 271, loss = 0.20055648\n",
      "Iteration 272, loss = 0.19918544\n",
      "Iteration 273, loss = 0.19770542\n",
      "Iteration 274, loss = 0.19618765\n",
      "Iteration 275, loss = 0.19484173\n",
      "Iteration 276, loss = 0.19356628\n",
      "Iteration 277, loss = 0.19228973\n",
      "Iteration 278, loss = 0.19105272\n",
      "Iteration 279, loss = 0.18983226\n",
      "Iteration 280, loss = 0.18863421\n",
      "Iteration 281, loss = 0.18737735\n",
      "Iteration 282, loss = 0.18609965\n",
      "Iteration 283, loss = 0.18488805\n",
      "Iteration 284, loss = 0.18401183\n",
      "Iteration 285, loss = 0.18296567\n",
      "Iteration 286, loss = 0.18166808\n",
      "Iteration 287, loss = 0.18008288\n",
      "Iteration 288, loss = 0.17868034\n",
      "Iteration 289, loss = 0.17767573\n",
      "Iteration 290, loss = 0.17651666\n",
      "Iteration 291, loss = 0.17551012\n",
      "Iteration 292, loss = 0.17428590\n",
      "Iteration 293, loss = 0.17308295\n",
      "Iteration 294, loss = 0.17175045\n",
      "Iteration 295, loss = 0.17047079\n",
      "Iteration 296, loss = 0.16940182\n",
      "Iteration 297, loss = 0.16836862\n",
      "Iteration 298, loss = 0.16726850\n",
      "Iteration 299, loss = 0.16603730\n",
      "Iteration 300, loss = 0.16489548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9610594518529717"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(50, 20), activation='relu', solver='adam', max_iter=300, verbose=True)\n",
    "mlp.fit(x_default_train, Y_train)\n",
    "\n",
    "Y_pred = mlp.predict(x_default_test)\n",
    "\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUPzNfG9XoHD"
   },
   "source": [
    "### For Discussion<br>\n",
    "Which model is better for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the most suitable model for a problem depends on factors like the coefficient of determination ($R^2$), complexity, and interpretability. From evaluating these metrics, we find that linear regression achieves a value of 0.898, whereas the single-layer MLP performs less favorably. Consequently, despite its greater complexity, the MLP with two hidden layers achieves an improved score of 0.948. Thus, the MLP with two hidden layers is deemed the most appropriate model, as it more effectively captures underlying patterns."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
